{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "785d111c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bs4 in c:\\users\\lavina\\appdata\\roaming\\python\\python39\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n"
     ]
    }
   ],
   "source": [
    "#install and import required libraries.\n",
    "!pip install bs4\n",
    "!pip install requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24720cbe",
   "metadata": {},
   "source": [
    "IMBD's top rated 100 movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2923208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie name</th>\n",
       "      <th>rating</th>\n",
       "      <th>year of release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>9.3</td>\n",
       "      <td>(1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>9.2</td>\n",
       "      <td>(1972)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>9</td>\n",
       "      <td>(1974)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>9</td>\n",
       "      <td>(2008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>9</td>\n",
       "      <td>(1957)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>North by Northwest</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(1959)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A Clockwork Orange</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(1971)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Snatch</td>\n",
       "      <td>8.2</td>\n",
       "      <td>(2000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Le fabuleux destin d'Amélie Poulain</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(2001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The Kid</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(1921)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             movie name rating year of release\n",
       "0              The Shawshank Redemption    9.3          (1994)\n",
       "1                         The Godfather    9.2          (1972)\n",
       "2                 The Godfather Part II      9          (1974)\n",
       "3                       The Dark Knight      9          (2008)\n",
       "4                          12 Angry Men      9          (1957)\n",
       "..                                  ...    ...             ...\n",
       "95                   North by Northwest    8.3          (1959)\n",
       "96                   A Clockwork Orange    8.3          (1971)\n",
       "97                               Snatch    8.2          (2000)\n",
       "98  Le fabuleux destin d'Amélie Poulain    8.3          (2001)\n",
       "99                              The Kid    8.3          (1921)\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#send get request to webpage to get source code\n",
    "page = requests.get(\"https://www.imdb.com/list/ls091520106/\")\n",
    "# page content\n",
    "soup = BeautifulSoup(page.content)   \n",
    "\n",
    "## define a function to scrap movie title\n",
    "def get_movie_titles(soup):\n",
    "    selection_class=\"lister-item-header\"\n",
    "    movie_title_tags=soup.find_all('h3',{'class':selection_class})\n",
    "    movie_titles=[]\n",
    "\n",
    "    for tag in movie_title_tags:\n",
    "        title = tag.find('a').text\n",
    "        movie_titles.append(title)\n",
    "    return movie_titles\n",
    "get_movie_titles(soup)\n",
    "\n",
    "#scraping a webpage to get rating and year of release\n",
    "rating = []      #empty list\n",
    "for i in soup.find_all('div', class_=\"ipl-rating-star small\"): ##for loop to scrap rating\n",
    "    rating.append(i.text.strip())\n",
    "rating\n",
    "\n",
    "year_of_release = []      #empty list\n",
    "for i in soup.find_all('span', class_=\"lister-item-year text-muted unbold\"):    ##for loop to scrap year of release\n",
    "    year_of_release.append(i.text)\n",
    "year_of_release\n",
    "\n",
    " ## define scraped data in data frame\n",
    "df = pd.DataFrame({' movie name':get_movie_titles(soup),'rating':rating,\"year of release\":year_of_release})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b61c54",
   "metadata": {},
   "source": [
    "IMBD's top rated 100 indian movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a07d187a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie name</th>\n",
       "      <th>rating</th>\n",
       "      <th>year of release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gangs of Wasseypur</td>\n",
       "      <td>8.2</td>\n",
       "      <td>(2012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Masaan</td>\n",
       "      <td>8.1</td>\n",
       "      <td>(2015)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ship of Theseus</td>\n",
       "      <td>8</td>\n",
       "      <td>(2012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Black Friday</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(2004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Udaan</td>\n",
       "      <td>8.1</td>\n",
       "      <td>(2010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Eeb Allay Ooo!</td>\n",
       "      <td>7.2</td>\n",
       "      <td>(2019)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Dasvidaniya</td>\n",
       "      <td>7.8</td>\n",
       "      <td>(2008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>A Wednesday</td>\n",
       "      <td>8.1</td>\n",
       "      <td>(2008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>LSD: Love, Sex Aur Dhokha</td>\n",
       "      <td>7.2</td>\n",
       "      <td>(2010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Trapped</td>\n",
       "      <td>7.5</td>\n",
       "      <td>(XVII) (2016)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   movie name rating year of release\n",
       "0          Gangs of Wasseypur    8.2          (2012)\n",
       "1                      Masaan    8.1          (2015)\n",
       "2             Ship of Theseus      8          (2012)\n",
       "3                Black Friday    8.4          (2004)\n",
       "4                       Udaan    8.1          (2010)\n",
       "..                        ...    ...             ...\n",
       "95             Eeb Allay Ooo!    7.2          (2019)\n",
       "96                Dasvidaniya    7.8          (2008)\n",
       "97                A Wednesday    8.1          (2008)\n",
       "98  LSD: Love, Sex Aur Dhokha    7.2          (2010)\n",
       "99                    Trapped    7.5   (XVII) (2016)\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = requests.get(\"https://www.imdb.com/list/ls561664100/\")  ##to check for scraping data\n",
    "soup = BeautifulSoup(page.content)   ##get page content in HTML\n",
    "## define a function to scrap movie title\n",
    "def get_movie_titles(soup):\n",
    "    selection_class=\"lister-item-header\"\n",
    "    movie_title_tags=soup.find_all('h3',{'class':selection_class})\n",
    "    movie_titles=[]\n",
    "\n",
    "    for tag in movie_title_tags:\n",
    "        title = tag.find('a').text\n",
    "        movie_titles.append(title)\n",
    "    return movie_titles\n",
    "get_movie_titles(soup)\n",
    "\n",
    "#scraping for rating\n",
    "rating = []      #empty list\n",
    "for i in soup.find_all('div', class_=\"ipl-rating-star small\"): ##for loop to scrap rating\n",
    "    rating.append(i.text.strip())\n",
    "rating\n",
    "\n",
    "#scraping for year of release\n",
    "year_of_release = []      #empty list\n",
    "for i in soup.find_all('span', class_=\"lister-item-year text-muted unbold\"):    ##for loop to scrap year of release\n",
    "    year_of_release.append(i.text)\n",
    "year_of_release\n",
    "\n",
    "## define scraped data in data frame\n",
    "df = pd.DataFrame({' movie name':get_movie_titles(soup),'rating':rating,\"year of release\":year_of_release})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc7071",
   "metadata": {},
   "source": [
    "cricket ranking from icc-cricket.com for mens team"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb28ebed",
   "metadata": {},
   "source": [
    "top 10 ODI team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef9d0647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of team</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>23</td>\n",
       "      <td>2,670</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>England</td>\n",
       "      <td>30</td>\n",
       "      <td>3,400</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Australia</td>\n",
       "      <td>32</td>\n",
       "      <td>3,572</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>38</td>\n",
       "      <td>4,098</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>22</td>\n",
       "      <td>2,354</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>24</td>\n",
       "      <td>2,392</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>33</td>\n",
       "      <td>3,129</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>31</td>\n",
       "      <td>2,800</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>20</td>\n",
       "      <td>1,419</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>41</td>\n",
       "      <td>2,902</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Name of team Matches Points Rating\n",
       "0   New Zealand      23  2,670    116\n",
       "1       England      30  3,400    113\n",
       "2     Australia      32  3,572    112\n",
       "3         India      38  4,098    108\n",
       "4      Pakistan      22  2,354    107\n",
       "5  South Africa      24  2,392    100\n",
       "6    Bangladesh      33  3,129     95\n",
       "7     Sri Lanka      31  2,800     90\n",
       "8   Afghanistan      20  1,419     71\n",
       "9   West Indies      41  2,902     71"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#send request to webpage server to get sourse code\n",
    "page = requests.get('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')\n",
    "soup = BeautifulSoup(page.content)\n",
    "# scraping first team\n",
    "first_team =soup.find('span', class_='u-hide-phablet')\n",
    "t1=first_team.text\n",
    "\n",
    "# scraping first match\n",
    "first_match =soup.find('td', class_='rankings-block__banner--matches')\n",
    "m1=first_match.text\n",
    "\n",
    "# scraping first points\n",
    "first_points =soup.find('td', class_='rankings-block__banner--points')\n",
    "p1=first_points.text\n",
    "\n",
    "# scraping first rating\n",
    "first_rating =soup.find('td', class_='rankings-block__banner--rating u-text-right')\n",
    "r1=first_rating.text.strip()\n",
    "# scraping multiple team\n",
    "team =[]   #empty list\n",
    "for i in soup.find_all('span', class_='u-hide-phablet'):\n",
    "    team.append(i.text)\n",
    "t=team[0:10]\n",
    "\n",
    "# scraping multiple matches\n",
    "name =[]  #empty list\n",
    "for i in soup.find_all('td', class_='table-body__cell u-center-text'):\n",
    "    name.append(i.text.strip())\n",
    "m2= name[0:38:2]\n",
    "m2.insert(0,m1)\n",
    "m=m2[0:10]\n",
    "\n",
    "# scraping multiple points\n",
    "p2=name[1:38:2]\n",
    "p2.insert(0,p1)\n",
    "p=p2[0:10]\n",
    "p\n",
    "\n",
    "# scraping multiple rating\n",
    "rating =[]    #empty list\n",
    "for i in soup.find_all('td', class_='table-body__cell u-text-right rating'):\n",
    "    rating.append(i.text)\n",
    "r=rating[0:9]\n",
    "r.insert(0,r1)\n",
    "# for checking length\n",
    "#print(len(t),len(m),len(p),len(r))\n",
    "# making dataframe\n",
    "\n",
    "df=pd.DataFrame({\"Name of team\":t,\"Matches\":m,\"Points\":p,\"Rating\":r})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a3a509",
   "metadata": {},
   "source": [
    "top 10 ODI batmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9e10789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Babar Azam</td>\n",
       "      <td>PAK</td>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Imam-ul-Haq</td>\n",
       "      <td>PAK</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rassie van der Dussen</td>\n",
       "      <td>SA</td>\n",
       "      <td>766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quinton de Kock</td>\n",
       "      <td>SA</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>David Warner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Steve Smith</td>\n",
       "      <td>AUS</td>\n",
       "      <td>719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jonny Bairstow</td>\n",
       "      <td>ENG</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>IND</td>\n",
       "      <td>707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rohit Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kane Williamson</td>\n",
       "      <td>NZ</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Name of player Team Rating\n",
       "0             Babar Azam  PAK    890\n",
       "1            Imam-ul-Haq  PAK    779\n",
       "2  Rassie van der Dussen   SA    766\n",
       "3        Quinton de Kock   SA    759\n",
       "4           David Warner  AUS    747\n",
       "5            Steve Smith  AUS    719\n",
       "6         Jonny Bairstow  ENG    710\n",
       "7            Virat Kohli  IND    707\n",
       "8           Rohit Sharma  IND    705\n",
       "9        Kane Williamson   NZ    700"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#send request to webpage server to get sourse code\n",
    "page = requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting')\n",
    "soup = BeautifulSoup(page.content)\n",
    "# scraping first name\n",
    "first_name =soup.find('div', class_='rankings-block__banner--name-large')\n",
    "n1=first_name.text\n",
    "\n",
    "# scraping first team\n",
    "first_team =soup.find('div', class_='rankings-block__banner--nationality')\n",
    "t1=first_team.text.strip()\n",
    "\n",
    "# scraping first rating\n",
    "first_rating =soup.find('div', class_='rankings-block__banner--rating')\n",
    "r1=first_rating.text\n",
    "# scraping multiple name\n",
    "name =[]  #empty list\n",
    "for i in soup.find_all('td', class_='table-body__cell rankings-table__name name'):\n",
    "    name.append(i.text.strip())\n",
    "n= name[0:9]\n",
    "n.insert(0,n1)\n",
    "\n",
    "# scraping multiple team\n",
    "team =[]   #empty list\n",
    "for i in soup.find_all('span', class_='table-body__logo-text'):\n",
    "    team.append(i.text)\n",
    "t=team[0:9]\n",
    "t.insert(0,t1)\n",
    "\n",
    "# scraping multiple rating\n",
    "rating =[]    #empty list\n",
    "for i in soup.find_all('td', class_='table-body__cell rating'):\n",
    "    rating.append(i.text)\n",
    "r=rating[0:9]\n",
    "r.insert(0,r1)\n",
    "# making dataframe\n",
    "df=pd.DataFrame({\"Name of player\":n,\"Team\":t,\"Rating\":r})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd7d78",
   "metadata": {},
   "source": [
    "top 10 ODI bowlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0868a170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of Bowler</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trent Boult</td>\n",
       "      <td>NZ</td>\n",
       "      <td>752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Josh Hazlewood</td>\n",
       "      <td>AUS</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mitchell Starc</td>\n",
       "      <td>AUS</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Matt Henry</td>\n",
       "      <td>NZ</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shaheen Afridi</td>\n",
       "      <td>PAK</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rashid Khan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Adam Zampa</td>\n",
       "      <td>AUS</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Shakib Al Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mustafizur Rahman</td>\n",
       "      <td>BAN</td>\n",
       "      <td>638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mujeeb Ur Rahman</td>\n",
       "      <td>AFG</td>\n",
       "      <td>637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name of Bowler Team Rating\n",
       "0        Trent Boult   NZ    752\n",
       "1     Josh Hazlewood  AUS    727\n",
       "2     Mitchell Starc  AUS    665\n",
       "3         Matt Henry   NZ    663\n",
       "4     Shaheen Afridi  PAK    661\n",
       "5        Rashid Khan  AFG    659\n",
       "6         Adam Zampa  AUS    655\n",
       "7    Shakib Al Hasan  BAN    652\n",
       "8  Mustafizur Rahman  BAN    638\n",
       "9   Mujeeb Ur Rahman  AFG    637"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#send request to webpage server to get sourse code\n",
    "page = requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling')\n",
    "soup = BeautifulSoup(page.content)\n",
    "# scraping first name\n",
    "first_name =soup.find('div', class_='rankings-block__banner--name-large')\n",
    "n1=first_name.text\n",
    "\n",
    "# scraping first team\n",
    "first_team =soup.find('div', class_='rankings-block__banner--nationality')\n",
    "t1=first_team.text.strip()\n",
    "\n",
    "# scraping first rating\n",
    "first_rating =soup.find('div', class_='rankings-block__banner--rating')\n",
    "r1=first_rating.text\n",
    "# scraping multiple name\n",
    "name =[]  #empty list\n",
    "for i in soup.find_all('td', class_='table-body__cell rankings-table__name name'):\n",
    "    name.append(i.text.strip())\n",
    "n= name[0:9]\n",
    "n.insert(0,n1)\n",
    "\n",
    "# scraping multiple team\n",
    "team =[]   #empty list\n",
    "for i in soup.find_all('span', class_='table-body__logo-text'):\n",
    "    team.append(i.text)\n",
    "t=team[0:9]\n",
    "t.insert(0,t1)\n",
    "\n",
    "# scraping multiple rating\n",
    "rating =[]    #empty list\n",
    "for i in soup.find_all('td', class_='table-body__cell rating'):\n",
    "    rating.append(i.text)\n",
    "r=rating[0:9]\n",
    "r.insert(0,r1)\n",
    "# making dataframe\n",
    "df=pd.DataFrame({\"Name of Bowler\":n,\"Team\":t,\"Rating\":r})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700fd1cb",
   "metadata": {},
   "source": [
    "Ranking from icc ricket.com for womens team "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7df65f2",
   "metadata": {},
   "source": [
    "Top 10 ODI womens team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89b7b833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of team</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NZ</td>\n",
       "      <td>23</td>\n",
       "      <td>2,670</td>\n",
       "      <td>752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AUS</td>\n",
       "      <td>30</td>\n",
       "      <td>3,400</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AUS</td>\n",
       "      <td>32</td>\n",
       "      <td>3,572</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NZ</td>\n",
       "      <td>38</td>\n",
       "      <td>4,098</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PAK</td>\n",
       "      <td>22</td>\n",
       "      <td>2,354</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AFG</td>\n",
       "      <td>24</td>\n",
       "      <td>2,392</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AUS</td>\n",
       "      <td>33</td>\n",
       "      <td>3,129</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BAN</td>\n",
       "      <td>31</td>\n",
       "      <td>2,800</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BAN</td>\n",
       "      <td>20</td>\n",
       "      <td>1,419</td>\n",
       "      <td>638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AFG</td>\n",
       "      <td>41</td>\n",
       "      <td>2,902</td>\n",
       "      <td>637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Name of team Matches Points Rating\n",
       "0           NZ      23  2,670    752\n",
       "1          AUS      30  3,400    727\n",
       "2          AUS      32  3,572    665\n",
       "3           NZ      38  4,098    663\n",
       "4          PAK      22  2,354    661\n",
       "5          AFG      24  2,392    659\n",
       "6          AUS      33  3,129    655\n",
       "7          BAN      31  2,800    652\n",
       "8          BAN      20  1,419    638\n",
       "9          AFG      41  2,902    637"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#send request to webpage server to get sourse code\n",
    "page = requests.get('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')\n",
    "soup = BeautifulSoup(page.content)\n",
    "# scraping first team\n",
    "first_team =soup.find('span', class_='u-hide-phablet')\n",
    "t1=first_team.text\n",
    "\n",
    "# scraping first match\n",
    "first_match =soup.find('td', class_='rankings-block__banner--matches')\n",
    "m1=first_match.text\n",
    "\n",
    "# scraping first points\n",
    "first_points =soup.find('td', class_='rankings-block__banner--points')\n",
    "p1=first_points.text\n",
    "\n",
    "# scraping first rating\n",
    "first_rating =soup.find('td', class_='rankings-block__banner--rating u-text-right')\n",
    "r1=first_rating.text.strip()\n",
    "# scraping first team\n",
    "first_team =soup.find('span', class_='u-hide-phablet')\n",
    "t1=first_team.text\n",
    "\n",
    "# scraping first match\n",
    "first_match =soup.find('td', class_='rankings-block__banner--matches')\n",
    "m1=first_match.text\n",
    "\n",
    "# scraping first points\n",
    "first_points =soup.find('td', class_='rankings-block__banner--points')\n",
    "p1=first_points.text\n",
    "\n",
    "# scraping first rating\n",
    "first_rating =soup.find('td', class_='rankings-block__banner--rating u-text-right')\n",
    "r1=first_rating.text.strip()\n",
    "# making dataframe\n",
    "df=pd.DataFrame({\"Name of team\":t,\"Matches\":m,\"Points\":p,\"Rating\":r})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0841d51",
   "metadata": {},
   "source": [
    "Top 10 womens ODI batting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2278dd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alyssa Healy</td>\n",
       "      <td>AUS</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beth Mooney</td>\n",
       "      <td>AUS</td>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Laura Wolvaardt</td>\n",
       "      <td>SA</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Harmanpreet Kaur</td>\n",
       "      <td>IND</td>\n",
       "      <td>716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Smriti Mandhana</td>\n",
       "      <td>IND</td>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Meg Lanning</td>\n",
       "      <td>AUS</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rachael Haynes</td>\n",
       "      <td>AUS</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chamari Athapaththu</td>\n",
       "      <td>SL</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Amy Satterthwaite</td>\n",
       "      <td>NZ</td>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Name of player Team Rating\n",
       "0         Alyssa Healy  AUS    785\n",
       "1          Beth Mooney  AUS    749\n",
       "2      Laura Wolvaardt   SA    732\n",
       "3       Natalie Sciver  ENG    731\n",
       "4     Harmanpreet Kaur  IND    716\n",
       "5      Smriti Mandhana  IND    714\n",
       "6          Meg Lanning  AUS    710\n",
       "7       Rachael Haynes  AUS    701\n",
       "8  Chamari Athapaththu   SL    655\n",
       "9    Amy Satterthwaite   NZ    654"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#send request to webpage server to get sourse code\n",
    "page = requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting')\n",
    "soup = BeautifulSoup(page.content)\n",
    "# scraping first name\n",
    "first_name =soup.find('div', class_='rankings-block__banner--name-large')\n",
    "n1=first_name.text\n",
    "\n",
    "# scraping first team\n",
    "first_team =soup.find('div', class_='rankings-block__banner--nationality')\n",
    "t1=first_team.text.strip()\n",
    "\n",
    "# scraping first rating\n",
    "first_rating =soup.find('div', class_='rankings-block__banner--rating')\n",
    "r1=first_rating.text\n",
    "# scraping multiple name\n",
    "name =[]  #empty list\n",
    "for i in soup.find_all('td', class_='table-body__cell rankings-table__name name'):\n",
    "    name.append(i.text.strip())\n",
    "n= name[0:9]\n",
    "n.insert(0,n1)\n",
    "\n",
    "# scraping multiple team\n",
    "team =[]   #empty list\n",
    "for i in soup.find_all('span', class_='table-body__logo-text'):\n",
    "    team.append(i.text)\n",
    "t=team[0:9]\n",
    "t.insert(0,t1)\n",
    "\n",
    "# scraping multiple rating\n",
    "rating =[]    #empty list\n",
    "for i in soup.find_all('td', class_='table-body__cell rating'):\n",
    "    rating.append(i.text)\n",
    "r=rating[0:9]\n",
    "r.insert(0,r1)\n",
    "#making dataframe\n",
    "df=pd.DataFrame({\"Name of player\":n,\"Team\":t,\"Rating\":r})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72242e",
   "metadata": {},
   "source": [
    "Top 10 womens ODI all rounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7893199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hayley Matthews</td>\n",
       "      <td>WI</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amelia Kerr</td>\n",
       "      <td>NZ</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Deepti Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ashleigh Gardner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jhulan Goswami</td>\n",
       "      <td>IND</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sophie Ecclestone</td>\n",
       "      <td>ENG</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name of player Team Rating\n",
       "0       Ellyse Perry  AUS    374\n",
       "1    Hayley Matthews   WI    373\n",
       "2     Natalie Sciver  ENG    371\n",
       "3     Marizanne Kapp   SA    349\n",
       "4        Amelia Kerr   NZ    343\n",
       "5      Deepti Sharma  IND    322\n",
       "6   Ashleigh Gardner  AUS    270\n",
       "7      Jess Jonassen  AUS    246\n",
       "8     Jhulan Goswami  IND    214\n",
       "9  Sophie Ecclestone  ENG    205"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#send request to webpage server to get sourse code\n",
    "page = requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder')\n",
    "soup = BeautifulSoup(page.content)\n",
    "# scraping first name\n",
    "first_name =soup.find('div', class_='rankings-block__banner--name-large')\n",
    "n1=first_name.text\n",
    "\n",
    "# scraping first team\n",
    "first_team =soup.find('div', class_='rankings-block__banner--nationality')\n",
    "t1=first_team.text.strip()\n",
    "\n",
    "# scraping first rating\n",
    "first_rating =soup.find('div', class_='rankings-block__banner--rating')\n",
    "r1=first_rating.text\n",
    "# scraping multiple name\n",
    "name =[]  #empty list\n",
    "for i in soup.find_all('td', class_='table-body__cell rankings-table__name name'):\n",
    "    name.append(i.text.strip())\n",
    "n= name[0:9]\n",
    "n.insert(0,n1)\n",
    "\n",
    "# scraping multiple team\n",
    "team =[]   #empty list\n",
    "for i in soup.find_all('span', class_='table-body__logo-text'):\n",
    "    team.append(i.text)\n",
    "t=team[0:9]\n",
    "t.insert(0,t1)\n",
    "\n",
    "# scraping multiple rating\n",
    "rating =[]    #empty list\n",
    "for i in soup.find_all('td', class_='table-body__cell rating'):\n",
    "    rating.append(i.text)\n",
    "r=rating[0:9]\n",
    "r.insert(0,r1)\n",
    "# making dataframe\n",
    "df=pd.DataFrame({\"Name of player\":n,\"Team\":t,\"Rating\":r})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3266b6",
   "metadata": {},
   "source": [
    "News details from cnbc.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d0e7e9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Time</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recession fears could trigger a 'lipstick' eff...</td>\n",
       "      <td>17 Min Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>European markets head for negative open after ...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Asia's year in review: Who had it good — and w...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Goldman Sachs names 3 stocks that could profit...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elon Musk sells another huge chunk of Tesla sh...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>China's retail sales shrink far more than expe...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FTX insider turned on Sam Bankman-Fried days b...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bank of America names the tech stocks set to b...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Missed China's reopening rally? BofA names glo...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cramer's lightning round: I wouldn't buy World...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Asia-Pacific markets trade lower after Fed's r...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Spooked investors have pulled billions of doll...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Jim Cramer says he likes these 5 consumer stap...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Investors bolting after hawkish Fed speech are...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Stock futures mixed following Fed update and a...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Pro Picks: Watch all of Wednesday's big stock ...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bond king Gundlach says the Fed should not do ...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Satellite imagery venture Planet sees record t...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Omicron subvariants are serious threat to boos...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Use this ‘4-part prescription’ to wake up aler...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Fusion power still decades and billions away, ...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Trump hits 7-year low in new national poll as ...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Tech group sues to block California law aiming...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>The secret to asking for a raise and getting i...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Why Europe's efforts to open up the iPhone won...</td>\n",
       "      <td>12 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Here are some of Credit Suisse's top stock pic...</td>\n",
       "      <td>12 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>We're initiating a small position in this Amer...</td>\n",
       "      <td>12 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Online tutoring side hustles are in demand, an...</td>\n",
       "      <td>12 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The Fed projects raising rates as high as 5.1%...</td>\n",
       "      <td>12 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The Fed expects more rate hikes, despite a sma...</td>\n",
       "      <td>12 Hours Ago</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Headline          Time   url\n",
       "0   Recession fears could trigger a 'lipstick' eff...    17 Min Ago  None\n",
       "1   European markets head for negative open after ...    1 Hour Ago  None\n",
       "2   Asia's year in review: Who had it good — and w...   2 Hours Ago  None\n",
       "3   Goldman Sachs names 3 stocks that could profit...   4 Hours Ago  None\n",
       "4   Elon Musk sells another huge chunk of Tesla sh...   5 Hours Ago  None\n",
       "5   China's retail sales shrink far more than expe...   5 Hours Ago  None\n",
       "6   FTX insider turned on Sam Bankman-Fried days b...   6 Hours Ago  None\n",
       "7   Bank of America names the tech stocks set to b...   7 Hours Ago  None\n",
       "8   Missed China's reopening rally? BofA names glo...   7 Hours Ago  None\n",
       "9   Cramer's lightning round: I wouldn't buy World...   7 Hours Ago  None\n",
       "10  Asia-Pacific markets trade lower after Fed's r...   8 Hours Ago  None\n",
       "11  Spooked investors have pulled billions of doll...   8 Hours Ago  None\n",
       "12  Jim Cramer says he likes these 5 consumer stap...   8 Hours Ago  None\n",
       "13  Investors bolting after hawkish Fed speech are...   8 Hours Ago  None\n",
       "14  Stock futures mixed following Fed update and a...   8 Hours Ago  None\n",
       "15  Pro Picks: Watch all of Wednesday's big stock ...   9 Hours Ago  None\n",
       "16  Bond king Gundlach says the Fed should not do ...  10 Hours Ago  None\n",
       "17  Satellite imagery venture Planet sees record t...  10 Hours Ago  None\n",
       "18  Omicron subvariants are serious threat to boos...  10 Hours Ago  None\n",
       "19  Use this ‘4-part prescription’ to wake up aler...  10 Hours Ago  None\n",
       "20  Fusion power still decades and billions away, ...  10 Hours Ago  None\n",
       "21  Trump hits 7-year low in new national poll as ...  10 Hours Ago  None\n",
       "22  Tech group sues to block California law aiming...  11 Hours Ago  None\n",
       "23  The secret to asking for a raise and getting i...  11 Hours Ago  None\n",
       "24  Why Europe's efforts to open up the iPhone won...  12 Hours Ago  None\n",
       "25  Here are some of Credit Suisse's top stock pic...  12 Hours Ago  None\n",
       "26  We're initiating a small position in this Amer...  12 Hours Ago  None\n",
       "27  Online tutoring side hustles are in demand, an...  12 Hours Ago  None\n",
       "28  The Fed projects raising rates as high as 5.1%...  12 Hours Ago  None\n",
       "29  The Fed expects more rate hikes, despite a sma...  12 Hours Ago  None"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the HTML\n",
    "page = requests.get('https://www.cnbc.com/world/?region=world')\n",
    "soup = BeautifulSoup(page.content)\n",
    "#scraping headlines\n",
    "Headline = []\n",
    "for i in soup.find_all('a',class_='LatestNews-headline'):\n",
    "    Headline.append(i.text)\n",
    "\n",
    "#scraping time\n",
    "Time = []\n",
    "for i in soup.find_all('time',class_='LatestNews-timestamp'):\n",
    "    Time.append(i.text)\n",
    "    \n",
    "\n",
    "# scraping the link\n",
    "url =[]\n",
    "for link in soup.find_all('a',class_='LatestNews-headline'):\n",
    "    url.append(i.get(\"https://www.cnbc.com/2022/12/15/european-markets-head-for-negative-open-after-hawkish-fed-comments.html\"))\n",
    "# printing url and length\n",
    "#print(url)\n",
    "#print(len(Headline),len(Time),len(link))\n",
    "# making dataframe\n",
    "df=pd.DataFrame({\"Headline\":Headline,\"Time\":Time,\"url\":url})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e32169",
   "metadata": {},
   "source": [
    "most downloaded article from journals.elseiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1de3f68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Paper URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>Bench-Capon, T.J.M., Dunne, Paul E.</td>\n",
       "      <td>October 2007</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>Bošanský, Branislav, Lisý, Viliam and 3 more</td>\n",
       "      <td>August 2016</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Luo, Wenhan, Xing, Junliang and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>Blum, Avrim L., Langley, Pat</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Arora, Saurabh, Doshi, Prashant</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Aas, Kjersti, Jullum, Martin, Løland, Anders</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.</td>\n",
       "      <td>December 2016</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>Riveiro, Maria, Thill, Serge</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...</td>\n",
       "      <td>January 2014</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Kohavi, Ron, John, George H.</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Quantum computation, quantum theory and AI</td>\n",
       "      <td>Ying, Mingsheng</td>\n",
       "      <td>February 2010</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Paper title  \\\n",
       "0                                    Reward is enough   \n",
       "1                           Making sense of raw input   \n",
       "2   Law and logic: A review from an argumentation ...   \n",
       "3              Creativity and artificial intelligence   \n",
       "4   Artificial cognition for social human–robot in...   \n",
       "5   Explanation in artificial intelligence: Insigh...   \n",
       "6                       Making sense of sensory input   \n",
       "7   Conflict-based search for optimal multi-agent ...   \n",
       "8   Between MDPs and semi-MDPs: A framework for te...   \n",
       "9   The Hanabi challenge: A new frontier for AI re...   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   \n",
       "11           Argumentation in artificial intelligence   \n",
       "12  Algorithms for computing strategies in two-pla...   \n",
       "13      Multiple object tracking: A literature review   \n",
       "14  Selection of relevant features and examples in...   \n",
       "15  A survey of inverse reinforcement learning: Ch...   \n",
       "16  Explaining individual predictions when feature...   \n",
       "17  A review of possible effects of cognitive bias...   \n",
       "18  Integrating social power into the decision-mak...   \n",
       "19  “That's (not) the output I expected!” On the r...   \n",
       "20  Explaining black-box classifiers using post-ho...   \n",
       "21  Algorithm runtime prediction: Methods & evalua...   \n",
       "22              Wrappers for feature subset selection   \n",
       "23  Commonsense visual sensemaking for autonomous ...   \n",
       "24         Quantum computation, quantum theory and AI   \n",
       "\n",
       "                                               Author  Published Date  \\\n",
       "0   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
       "1           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
       "2                   Prakken, Henry, Sartor, Giovanni     October 2015   \n",
       "3                                 Boden, Margaret A.      August 1998   \n",
       "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
       "5                                        Miller, Tim    February 2019   \n",
       "6   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
       "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
       "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
       "9         Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
       "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
       "11               Bench-Capon, T.J.M., Dunne, Paul E.     October 2007   \n",
       "12       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
       "13             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
       "14                      Blum, Avrim L., Langley, Pat    December 1997   \n",
       "15                   Arora, Saurabh, Doshi, Prashant      August 2021   \n",
       "16      Aas, Kjersti, Jullum, Martin, Løland, Anders   September 2021   \n",
       "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
       "18    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   \n",
       "19                      Riveiro, Maria, Thill, Serge   September 2021   \n",
       "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
       "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
       "22                      Kohavi, Ron, John, George H.    December 1997   \n",
       "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
       "24                                   Ying, Mingsheng    February 2010   \n",
       "\n",
       "   Paper URL  \n",
       "0       None  \n",
       "1       None  \n",
       "2       None  \n",
       "3       None  \n",
       "4       None  \n",
       "5       None  \n",
       "6       None  \n",
       "7       None  \n",
       "8       None  \n",
       "9       None  \n",
       "10      None  \n",
       "11      None  \n",
       "12      None  \n",
       "13      None  \n",
       "14      None  \n",
       "15      None  \n",
       "16      None  \n",
       "17      None  \n",
       "18      None  \n",
       "19      None  \n",
       "20      None  \n",
       "21      None  \n",
       "22      None  \n",
       "23      None  \n",
       "24      None  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#send request to webpage server to get sourse code\n",
    "page = requests.get('https://journals.elsevier.com/artificial-intelligence/most-downloaded-articles')\n",
    "soup = BeautifulSoup(page.content)\n",
    "# scraping multiple Paper_Title\n",
    "Paper_Title =[]  #empty list\n",
    "for i in soup.find_all('h2', class_='sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg'):\n",
    "    Paper_Title.append(i.text.strip())\n",
    "Paper_Title\n",
    "# scraping multiple Authors\n",
    "Authors =[]   #empty list\n",
    "for i in soup.find_all('span', class_='sc-1w3fpd7-0 dnCnAO'):\n",
    "    Authors.append(i.text)\n",
    "Authors\n",
    "# scraping multiple Published_Date\n",
    "Published_Date =[]    #empty list\n",
    "for i in soup.find_all('span', class_='sc-1thf9ly-2 dvggWt'):\n",
    "    Published_Date.append(i.text)\n",
    "Published_Date\n",
    "# scraping url\n",
    "url=[]\n",
    "for i in soup.find_all('span',class_='sc-1thf9ly-2 dvggWt'):\n",
    "    url.append(i.get('https://www.sciencedirect.com/science/article/pii/S0004370221000862'))\n",
    "# printing length\n",
    "#print(len(Paper_Title),len(Authors),len(Published_Date),len(url))\n",
    "# making dataframe\n",
    "df=pd.DataFrame({'Paper title':Paper_Title,'Author':Authors,'Published Date':Published_Date,'Paper URL':url})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5a993f",
   "metadata": {},
   "source": [
    "details from dineout.co.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30a37444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of restaurant</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Location</th>\n",
       "      <th>Image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tamasha</td>\n",
       "      <td>₹ 2,000 for 2 (approx) | Continental, Asian, I...</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Local</td>\n",
       "      <td>₹ 2,000 for 2 (approx) | North Indian, Asian, ...</td>\n",
       "      <td>Scindia House,Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Junkyard Cafe</td>\n",
       "      <td>₹ 2,100 for 2 (approx) | North Indian, Contine...</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Station Bar</td>\n",
       "      <td>₹ 1,100 for 2 (approx) | Italian, Chinese, Nor...</td>\n",
       "      <td>F-Block,Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ministry Of Beer</td>\n",
       "      <td>₹ 3,000 for 2 (approx) | North Indian, Contine...</td>\n",
       "      <td>M-Block,Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Unplugged Courtyard</td>\n",
       "      <td>₹ 3,300 for 2 (approx) | North Indian, Italian...</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>QBA</td>\n",
       "      <td>₹ 2,100 for 2 (approx) | North Indian, Contine...</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>My Bar Square</td>\n",
       "      <td>₹ 2,000 for 2 (approx) | Finger Food, Chinese,...</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The G.T. ROAD</td>\n",
       "      <td>₹ 1,400 for 2 (approx) | North Indian</td>\n",
       "      <td>M-Block,Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Openhouse Cafe</td>\n",
       "      <td>₹ 2,000 for 2 (approx) | North Indian, Asian, ...</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Warehouse Cafe</td>\n",
       "      <td>₹ 2,500 for 2 (approx) | North Indian, Chinese...</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dasaprakash</td>\n",
       "      <td>₹ 800 for 2 (approx) | North Indian, South Ind...</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Luggage Room By Sandoz</td>\n",
       "      <td>₹ 1,600 for 2 (approx) | Chinese, Italian, Nor...</td>\n",
       "      <td>M-Block,Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Lord of the Drinks</td>\n",
       "      <td>₹ 2,500 for 2 (approx) | Chinese, North Indian...</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Imperial Spice</td>\n",
       "      <td>₹ 3,000 for 2 (approx) | North Indian, Chinese...</td>\n",
       "      <td>M-Block,Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Somewhere Restaurant &amp; Bar</td>\n",
       "      <td>₹ 1,000 for 2 (approx) | North Indian, Contine...</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Connaught Clubhouse Microbrewery</td>\n",
       "      <td>₹ 1,800 for 2 (approx) | North Indian, Contine...</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ardor 2.1 Restaurant and Lounge</td>\n",
       "      <td>₹ 2,000 for 2 (approx) | North Indian, Chinese...</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Cafe High 5</td>\n",
       "      <td>₹ 1,700 for 2 (approx) | North Indian, Contine...</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>My Bar Headquarters</td>\n",
       "      <td>₹ 1,500 for 2 (approx) | North Indian, Chinese</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chido</td>\n",
       "      <td>₹ 1,800 for 2 (approx) | North Indian, Italian...</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name of restaurant  \\\n",
       "0                            Tamasha   \n",
       "1                              Local   \n",
       "2                  The Junkyard Cafe   \n",
       "3                        Station Bar   \n",
       "4                   Ministry Of Beer   \n",
       "5                Unplugged Courtyard   \n",
       "6                                QBA   \n",
       "7                      My Bar Square   \n",
       "8                      The G.T. ROAD   \n",
       "9                     Openhouse Cafe   \n",
       "10                    Warehouse Cafe   \n",
       "11                       Dasaprakash   \n",
       "12        The Luggage Room By Sandoz   \n",
       "13                Lord of the Drinks   \n",
       "14                The Imperial Spice   \n",
       "15        Somewhere Restaurant & Bar   \n",
       "16  Connaught Clubhouse Microbrewery   \n",
       "17   Ardor 2.1 Restaurant and Lounge   \n",
       "18                       Cafe High 5   \n",
       "19               My Bar Headquarters   \n",
       "20                             Chido   \n",
       "\n",
       "                                              Cuisine  \\\n",
       "0   ₹ 2,000 for 2 (approx) | Continental, Asian, I...   \n",
       "1   ₹ 2,000 for 2 (approx) | North Indian, Asian, ...   \n",
       "2   ₹ 2,100 for 2 (approx) | North Indian, Contine...   \n",
       "3   ₹ 1,100 for 2 (approx) | Italian, Chinese, Nor...   \n",
       "4   ₹ 3,000 for 2 (approx) | North Indian, Contine...   \n",
       "5   ₹ 3,300 for 2 (approx) | North Indian, Italian...   \n",
       "6   ₹ 2,100 for 2 (approx) | North Indian, Contine...   \n",
       "7   ₹ 2,000 for 2 (approx) | Finger Food, Chinese,...   \n",
       "8               ₹ 1,400 for 2 (approx) | North Indian   \n",
       "9   ₹ 2,000 for 2 (approx) | North Indian, Asian, ...   \n",
       "10  ₹ 2,500 for 2 (approx) | North Indian, Chinese...   \n",
       "11  ₹ 800 for 2 (approx) | North Indian, South Ind...   \n",
       "12  ₹ 1,600 for 2 (approx) | Chinese, Italian, Nor...   \n",
       "13  ₹ 2,500 for 2 (approx) | Chinese, North Indian...   \n",
       "14  ₹ 3,000 for 2 (approx) | North Indian, Chinese...   \n",
       "15  ₹ 1,000 for 2 (approx) | North Indian, Contine...   \n",
       "16  ₹ 1,800 for 2 (approx) | North Indian, Contine...   \n",
       "17  ₹ 2,000 for 2 (approx) | North Indian, Chinese...   \n",
       "18  ₹ 1,700 for 2 (approx) | North Indian, Contine...   \n",
       "19     ₹ 1,500 for 2 (approx) | North Indian, Chinese   \n",
       "20  ₹ 1,800 for 2 (approx) | North Indian, Italian...   \n",
       "\n",
       "                                        Location  \\\n",
       "0                 Connaught Place, Central Delhi   \n",
       "1   Scindia House,Connaught Place, Central Delhi   \n",
       "2                 Connaught Place, Central Delhi   \n",
       "3         F-Block,Connaught Place, Central Delhi   \n",
       "4         M-Block,Connaught Place, Central Delhi   \n",
       "5                 Connaught Place, Central Delhi   \n",
       "6                 Connaught Place, Central Delhi   \n",
       "7                 Connaught Place, Central Delhi   \n",
       "8         M-Block,Connaught Place, Central Delhi   \n",
       "9                 Connaught Place, Central Delhi   \n",
       "10                Connaught Place, Central Delhi   \n",
       "11                Connaught Place, Central Delhi   \n",
       "12        M-Block,Connaught Place, Central Delhi   \n",
       "13                Connaught Place, Central Delhi   \n",
       "14        M-Block,Connaught Place, Central Delhi   \n",
       "15                Connaught Place, Central Delhi   \n",
       "16                Connaught Place, Central Delhi   \n",
       "17                Connaught Place, Central Delhi   \n",
       "18                Connaught Place, Central Delhi   \n",
       "19                Connaught Place, Central Delhi   \n",
       "20                Connaught Place, Central Delhi   \n",
       "\n",
       "                                                Image  \n",
       "0   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "9   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "10  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "11  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "12  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "13  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "14  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "15  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "16  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "17  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "18  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "19  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "20  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = requests.get(\"https://www.dineout.co.in/delhi-restaurants\")\n",
    "soup = BeautifulSoup(page.content)\n",
    "## scraping multiple titles\n",
    "tittle = []  #empty list\n",
    "for i in soup.find_all('a', class_='restnt-name ellipsis'):\n",
    "    tittle.append(i.text)\n",
    "\n",
    "## scraping multiple cuisine\n",
    "cuisine = []  #empty list\n",
    "for i in soup.find_all('span', class_='double-line-ellipsis'):\n",
    "    cuisine.append(i.text)\n",
    "\n",
    "    ## scraping multiple location\n",
    "location = []  #empty list\n",
    "for i in soup.find_all('div', class_='restnt-loc ellipsis'):\n",
    "    location.append(i.text)\n",
    "    \n",
    "## scraping multiple rating\n",
    "rating = []  #empty list\n",
    "for i in soup.find_all('div', class_='restnt-rating rating-4'):\n",
    "    rating.append(i.text)\n",
    "    \n",
    "## scraping image\n",
    "image = []  #empty list\n",
    "for i in soup.find_all('img', class_='no-img'):\n",
    "    image.append(i.get('data-src'))\n",
    "    \n",
    "# making dataframe\n",
    "df =pd.DataFrame({'Name of restaurant':tittle,'Cuisine':cuisine,'Location':location,'Image':image})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65ebeac",
   "metadata": {},
   "source": [
    "Top publication from scholar google form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a801e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the HTML\n",
    "page = requests.get('https://scholar.google.co.in/citations?view_op=top_venues&hl=en')\n",
    "soup = BeautifulSoup(page.content)\n",
    "#scraping rank\n",
    "Rank=[]\n",
    "for i in soup.find_all('th', class_='gsc_mvt_p'):\n",
    "    Rank.append(i.text)\n",
    "\n",
    "#scraping publication\n",
    "Publication=[]\n",
    "for i in soup.find_all('th', class_='gsc_mvt_t'):\n",
    "    Publication.append(i.text)\n",
    "\n",
    "#scraping h-5 index\n",
    "h5_ind=[]\n",
    "for i in soup.find_all('th', class_='gsc_mvt_n'):\n",
    "    h5_ind.append(i.text)\n",
    "    \n",
    "h5_index=h5_ind[: :2]\n",
    "h5_median=h5_ind[1: :2]\n",
    "\n",
    "#printing length\n",
    "print(len(Rank),len(Publication),len(h5_index),len(h5_median))\n",
    "\n",
    "df= pd.DataFrame({'Rank':Rank,'Publication':Publication,'h5_index':h5_index,'h5_median':h5_median})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e48ccf",
   "metadata": {},
   "source": [
    "Former president of india"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f004d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "president name \n",
      "Shri Ram Nath Kovind (birth - 1945)\n",
      "Term of Office: 25 July, 2017 to 25 July, 2022 \n",
      "https://ramnathkovind.nic.in\n",
      " \n",
      "Shri Pranab Mukherjee (1935-2020)\n",
      "Term of Office: 25 July, 2012 to 25 July, 2017 \n",
      "http://pranabmukherjee.nic.in\n",
      " \n",
      "Smt Pratibha Devisingh Patil (birth - 1934)\n",
      "Term of Office: 25 July, 2007 to 25 July, 2012 \n",
      "http://pratibhapatil.nic.in\n",
      " \n",
      "DR. A.P.J. Abdul Kalam (1931-2015)\n",
      "Term of Office: 25 July, 2002 to 25 July, 2007 \n",
      "http://abdulkalam.nic.in\n",
      " \n",
      "Shri K. R. Narayanan (1920 - 2005)\n",
      "Term of Office: 25 July, 1997 to 25 July, 2002 \n",
      " \n",
      "Dr Shankar Dayal Sharma (1918-1999)\n",
      "Term of Office: 25 July, 1992 to 25 July, 1997 \n",
      " \n",
      "Shri R Venkataraman (1910-2009)\n",
      "Term of Office: 25 July, 1987 to 25 July, 1992 \n",
      " \n",
      "Giani Zail Singh (1916-1994)\n",
      "Term of Office: 25 July, 1982 to 25 July, 1987 \n",
      " \n",
      "Shri Neelam Sanjiva Reddy (1913-1996)\n",
      "Term of Office: 25 July, 1977 to 25 July, 1982 \n",
      " \n",
      "Dr. Fakhruddin Ali Ahmed (1905-1977)\n",
      "Term of Office: 24 August, 1974 to 11 February, 1977\n",
      " \n",
      "Shri Varahagiri Venkata Giri (1894-1980)\n",
      "Term of Office: 3 May, 1969 to 20 July, 1969 and 24 August, 1969 to 24 August, 1974\n",
      " \n",
      "Dr. Zakir Husain (1897-1969)\n",
      "Term of Office: 13 May, 1967 to 3 May, 1969\n",
      " \n",
      "Dr. Sarvepalli Radhakrishnan (1888-1975)\n",
      "Term of Office: 13 May, 1962 to 13 May, 1967\n",
      " \n",
      "Dr. Rajendra Prasad (1884-1963) \n",
      "Term of Office: 26 January, 1950 to 13 May, 1962\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\nShri Ram Nath Kovind (birth - 1945)\\nTerm of Office: 25 July, 2017 to 25 July, 2022 \\nhttps://ramnathkovind.nic.in\\n',\n",
       " '\\nShri Pranab Mukherjee (1935-2020)\\nTerm of Office: 25 July, 2012 to 25 July, 2017 \\nhttp://pranabmukherjee.nic.in\\n',\n",
       " '\\nSmt Pratibha Devisingh Patil (birth - 1934)\\nTerm of Office: 25 July, 2007 to 25 July, 2012 \\nhttp://pratibhapatil.nic.in\\n',\n",
       " '\\nDR. A.P.J. Abdul Kalam (1931-2015)\\nTerm of Office: 25 July, 2002 to 25 July, 2007 \\nhttp://abdulkalam.nic.in\\n',\n",
       " '\\nShri K. R. Narayanan (1920 - 2005)\\nTerm of Office: 25 July, 1997 to 25 July, 2002 \\n',\n",
       " '\\nDr Shankar Dayal Sharma (1918-1999)\\nTerm of Office: 25 July, 1992 to 25 July, 1997 \\n',\n",
       " '\\nShri R Venkataraman (1910-2009)\\nTerm of Office: 25 July, 1987 to 25 July, 1992 \\n',\n",
       " '\\nGiani Zail Singh (1916-1994)\\nTerm of Office: 25 July, 1982 to 25 July, 1987 \\n',\n",
       " '\\nShri Neelam Sanjiva Reddy (1913-1996)\\nTerm of Office: 25 July, 1977 to 25 July, 1982 \\n',\n",
       " '\\nDr. Fakhruddin Ali Ahmed (1905-1977)\\nTerm of Office: 24 August, 1974 to 11 February, 1977\\n',\n",
       " '\\nShri Varahagiri Venkata Giri (1894-1980)\\nTerm of Office: 3 May, 1969 to 20 July, 1969 and 24 August, 1969 to 24 August, 1974\\n',\n",
       " '\\nDr. Zakir Husain (1897-1969)\\nTerm of Office: 13 May, 1967 to 3 May, 1969\\n',\n",
       " '\\nDr. Sarvepalli Radhakrishnan (1888-1975)\\nTerm of Office: 13 May, 1962 to 13 May, 1967\\n',\n",
       " '\\nDr. Rajendra Prasad (1884-1963) \\nTerm of Office: 26 January, 1950 to 13 May, 1962\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send get request to webpage server to get source code\n",
    "page = requests.get(\"https://presidentofindia.nic.in/former-presidents.htm\") #\n",
    "# get page content\n",
    "soup = BeautifulSoup(page.content)\n",
    "# sraping data\n",
    "president_name= []\n",
    "for i in soup.find_all('div', class_='presidentListing'):\n",
    "    president_name.append(i.text)\n",
    "print('president name',*president_name, sep=' ')\n",
    "    \n",
    "president_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2556dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
